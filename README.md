
# ğŸ§  MemoryGPT

A conversational memory-augmented AI system that allows users to upload documents and query them in real-time â€” where your queries live forever.

---



## ğŸ›  Tech Stack

- **Frontend:** React.js, Tailwind CSS, Lucide Icons
- **Backend:** Flask, LangChain, FAISS, Cohere/OpenAI embeddings
- **Vector Store:** FAISS (stored locally)
- **PDF Parsing:** PyMuPDF, Unstructured
- **Deployment:** Vercel (Frontend), Render (Backend)

---

## ğŸ“¦ Installation

### ğŸ”§ Backend Setup

```bash
# Clone the repository
git clone https://github.com/jxhn13/memorygpt.git
cd memorygpt/backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Or venv\Scripts\activate on Windows

# Install requirements
pip install -r requirements.txt
````

Create a `.env` file:

```env
COHERE_API_KEY=your_cohere_api_key
# or
OPENAI_API_KEY=your_openai_api_key
EMBEDDING_MODEL=embed-english-light-v3.0
```

---

### ğŸ’» Frontend Setup

```bash
cd ../frontend
npm install
npm run dev  # Runs on http://localhost:5173
```

Update `API_BASE` in `src/Pages/Home.jsx`:

```js
const API_BASE = "http://localhost:5000"; // For local
// Or use Render URL in production
```

---

## ğŸ§  Features

* ğŸ“ Upload PDFs, DOCX, and TXT files
* ğŸ” Ask contextual questions from documents
* ğŸ§© Visual memory trail of chat history
* ğŸ“Œ Persistent vector store (FAISS)
* ğŸ¯ Tag extraction using KeyBERT
* ğŸš« Memory clearing and file deletion
* âœ¨ Immersive UI with animated chat

---

## ğŸ§ª API Endpoints

| Method | Endpoint            | Description             |
| ------ | ------------------- | ----------------------- |
| POST   | `/api/chat`         | Query with question     |
| POST   | `/api/upload`       | Upload a document       |
| POST   | `/api/delete-file`  | Delete uploaded file    |
| POST   | `/api/clear-memory` | Clear all stored memory |
| GET    | `/`                 | Health check route      |

---

## âš™ï¸ Deployment

### Render (Backend)

1. Create a new **Web Service**
2. Add build command: `pip install -r requirements.txt`
3. Start command: `python app.py`
4. Add environment variable: `PORT = 10000`, `COHERE_API_KEY`, etc.
5. Make sure `vector_store/` is committed if you skip preloading

### Vercel (Frontend)

1. Link GitHub repo to Vercel
2. Set `VITE_API_BASE` in Vercel Dashboard
3. Deploy ğŸš€

---

## ğŸ“ Folder Structure

```
memorygpt/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/              # Flask API routes
â”‚   â”œâ”€â”€ services/         # RAG logic, embedding, file handling
â”‚   â”œâ”€â”€ utils/            # Helpers and memory functions
â”‚   â”œâ”€â”€ vector_store/     # FAISS index files
â”‚   â”œâ”€â”€ app.py            # Main backend app
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ vite.config.js
```

---

## ğŸ™‹â€â™‚ï¸ Author

**John Shajan**
GitHub: [@jxhn13](https://github.com/jxhn13)
LinkedIn: [linkedin.com/in/johnshajan](https://linkedin.com/in/johnshajan)

---

## âš–ï¸ License

This project is licensed under the [MIT License](LICENSE).

````




